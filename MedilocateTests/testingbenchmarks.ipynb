{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e192455",
   "metadata": {},
   "source": [
    "just included some potential metrics to evaluate on so we can have a discussion section on evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd66407",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install textstat rouge-score bert-score spacy scispacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import spacy\n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "\n",
    "# Load English model for entity extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # For medical, try scispacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc5a2a",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Semantic Coverage (ROUGE + BERTScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13959208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(source_text, generated_text)\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score.score([generated_text], [source_text], lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "print(\"\\nüîç Semantic Coverage:\")\n",
    "print(\"ROUGE-1 Recall:\", rouge_scores['rouge1'].recall)\n",
    "print(\"BERTScore F1:\", F1[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6937e3f",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Information Reliability (Entity Overlap Heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f54a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return set([ent.text.lower() for ent in doc.ents])\n",
    "\n",
    "source_ents = extract_entities(source_text)\n",
    "gen_ents = extract_entities(generated_text)\n",
    "\n",
    "intersection = source_ents.intersection(gen_ents)\n",
    "reliability_score = len(intersection) / max(1, len(gen_ents))\n",
    "\n",
    "print(\"\\n‚úÖ Information Reliability:\")\n",
    "print(\"Extracted Entities in Source:\", source_ents)\n",
    "print(\"Entities in Generated Output:\", gen_ents)\n",
    "print(\"Reliability Score:\", round(reliability_score, 2))  # heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77337bb",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Readability (textstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìò Readability Stats:\")\n",
    "print(\"Flesch-Kincaid Grade Level:\", textstat.flesch_kincaid_grade(generated_text))\n",
    "print(\"Reading Ease Score:\", textstat.flesch_reading_ease(generated_text))\n",
    "print(\"SMOG Index:\", textstat.smog_index(generated_text))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
